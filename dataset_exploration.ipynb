{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Requirements"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a43ea327c14e3a2e"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "import requests\n",
    "import gzip\n",
    "from IPython.display import Image\n",
    "from VGG import get_image_embeddings, get_similarity_score, load_image_bytes"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-26T11:33:40.979860800Z",
     "start_time": "2024-06-26T11:33:27.953832300Z"
    }
   },
   "id": "18a54e084db38531",
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Unzipping the dataset"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9ba22b1621347e6b"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "with zipfile.ZipFile(\"Datasets/archive.zip\", 'r') as zip_ref:\n",
    "    zip_ref.extractall(\"Datasets\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-26T07:25:28.907355400Z",
     "start_time": "2024-06-26T07:25:25.927760800Z"
    }
   },
   "id": "94cd6bc1f14cf2f",
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Reading the csv"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9125706d86594f1f"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "path = os.getcwd() + \"/Datasets\"\n",
    "csv_files = glob.glob(os.path.join(path, \"*.csv\"))\n",
    "csv_dfs = {}\n",
    "for f in csv_files:\n",
    "    file_name = f.split(\"\\\\\")[-1]\n",
    "    \n",
    "    # remove the .csv extension\n",
    "    file_name = file_name[:-4]\n",
    "    \n",
    "    # read the csv file \n",
    "    df = pd.read_csv(f)\n",
    "    csv_dfs[file_name] = df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-26T11:33:54.244470700Z",
     "start_time": "2024-06-26T11:33:44.637468800Z"
    }
   },
   "id": "d17f8a8cd5c8c7c8",
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Usage"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9ccce3e2eb6033cc"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "                                                   name main_category  \\\n0     Aqualogica Glow+ Dewy Sunscreen SPF 50 PA+++ F...        stores   \n1     MARVIK Soft Silicone Adjustable Band Strap Com...        stores   \n2     108 Panchatantra Stories for Children (Illustr...        stores   \n3     Black and White and Blue: Adult Cinema From th...        stores   \n4     SAFARI 15 Ltrs Sea Blue Casual/School/College ...        stores   \n...                                                 ...           ...   \n2347  Lakme Lip Love Gelato Lip Balm - Pink, Bubbleg...        stores   \n2348  SATTVA Classy.Elegant.Stylish Classic XXXL Bea...        stores   \n2349  Fastrack Glitch Analog Rose Gold Dial Women's ...        stores   \n2350  NAINVISH Women/Girl's Pure Cotton Paisley Prin...        stores   \n2351  Tupperware Aquasafe Xtreme Plastic Bottle, 1L,...        stores   \n\n        sub_category                                              image  \\\n0     Amazon Fashion  https://m.media-amazon.com/images/I/51TSC6Uogx...   \n1     Amazon Fashion  https://m.media-amazon.com/images/I/51vYDop04S...   \n2     Amazon Fashion  https://m.media-amazon.com/images/I/81VJ+MAc7Y...   \n3     Amazon Fashion  https://m.media-amazon.com/images/I/51RIhaW3t1...   \n4     Amazon Fashion  https://m.media-amazon.com/images/I/61kmCas5OC...   \n...              ...                                                ...   \n2347  Amazon Fashion  https://m.media-amazon.com/images/I/61lc+uAfF0...   \n2348  Amazon Fashion  https://m.media-amazon.com/images/I/71zaq78pO8...   \n2349  Amazon Fashion  https://m.media-amazon.com/images/I/61B+fqeyzj...   \n2350  Amazon Fashion  https://m.media-amazon.com/images/I/81qATlsYzk...   \n2351  Amazon Fashion  https://m.media-amazon.com/images/I/51z7AvKIml...   \n\n                                                   link ratings no_of_ratings  \\\n0     https://www.amazon.in/Aqualogica-Sunscreen-Pro...     4.2         3,628   \n1     https://www.amazon.in/MARVIK-Silicone-Adjustab...     3.9           291   \n2     https://www.amazon.in/108-Panchatantra-Stories...     4.4         2,950   \n3     https://www.amazon.in/Black-White-Blue-Cinema-...     4.2             8   \n4     https://www.amazon.in/SAFARI-Ltrs-Casual-Backp...     4.0        17,985   \n...                                                 ...     ...           ...   \n2347  https://www.amazon.in/LAKM%C3%89-Love-Gelato-C...     4.1           515   \n2348  https://www.amazon.in/Sattva-Classic-Filled-Be...     4.2            10   \n2349  https://www.amazon.in/Fastrack-Glitch-Analog-W...     3.5            16   \n2350  https://www.amazon.in/NAINVISH-Cotton-Paisley-...     3.4           318   \n2351  https://www.amazon.in/Tupperware-Aquasafe-Xtre...     4.6         2,741   \n\n     discount_price actual_price  \n0              ₹351         ₹399  \n1              ₹249         ₹999  \n2              ₹125         ₹160  \n3               NaN      ₹584.16  \n4              ₹299         ₹799  \n...             ...          ...  \n2347           ₹148         ₹199  \n2348         ₹2,170       ₹4,299  \n2349            NaN       ₹5,595  \n2350           ₹499       ₹2,599  \n2351           ₹666         ₹695  \n\n[2352 rows x 9 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>name</th>\n      <th>main_category</th>\n      <th>sub_category</th>\n      <th>image</th>\n      <th>link</th>\n      <th>ratings</th>\n      <th>no_of_ratings</th>\n      <th>discount_price</th>\n      <th>actual_price</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Aqualogica Glow+ Dewy Sunscreen SPF 50 PA+++ F...</td>\n      <td>stores</td>\n      <td>Amazon Fashion</td>\n      <td>https://m.media-amazon.com/images/I/51TSC6Uogx...</td>\n      <td>https://www.amazon.in/Aqualogica-Sunscreen-Pro...</td>\n      <td>4.2</td>\n      <td>3,628</td>\n      <td>₹351</td>\n      <td>₹399</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>MARVIK Soft Silicone Adjustable Band Strap Com...</td>\n      <td>stores</td>\n      <td>Amazon Fashion</td>\n      <td>https://m.media-amazon.com/images/I/51vYDop04S...</td>\n      <td>https://www.amazon.in/MARVIK-Silicone-Adjustab...</td>\n      <td>3.9</td>\n      <td>291</td>\n      <td>₹249</td>\n      <td>₹999</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>108 Panchatantra Stories for Children (Illustr...</td>\n      <td>stores</td>\n      <td>Amazon Fashion</td>\n      <td>https://m.media-amazon.com/images/I/81VJ+MAc7Y...</td>\n      <td>https://www.amazon.in/108-Panchatantra-Stories...</td>\n      <td>4.4</td>\n      <td>2,950</td>\n      <td>₹125</td>\n      <td>₹160</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Black and White and Blue: Adult Cinema From th...</td>\n      <td>stores</td>\n      <td>Amazon Fashion</td>\n      <td>https://m.media-amazon.com/images/I/51RIhaW3t1...</td>\n      <td>https://www.amazon.in/Black-White-Blue-Cinema-...</td>\n      <td>4.2</td>\n      <td>8</td>\n      <td>NaN</td>\n      <td>₹584.16</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>SAFARI 15 Ltrs Sea Blue Casual/School/College ...</td>\n      <td>stores</td>\n      <td>Amazon Fashion</td>\n      <td>https://m.media-amazon.com/images/I/61kmCas5OC...</td>\n      <td>https://www.amazon.in/SAFARI-Ltrs-Casual-Backp...</td>\n      <td>4.0</td>\n      <td>17,985</td>\n      <td>₹299</td>\n      <td>₹799</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>2347</th>\n      <td>Lakme Lip Love Gelato Lip Balm - Pink, Bubbleg...</td>\n      <td>stores</td>\n      <td>Amazon Fashion</td>\n      <td>https://m.media-amazon.com/images/I/61lc+uAfF0...</td>\n      <td>https://www.amazon.in/LAKM%C3%89-Love-Gelato-C...</td>\n      <td>4.1</td>\n      <td>515</td>\n      <td>₹148</td>\n      <td>₹199</td>\n    </tr>\n    <tr>\n      <th>2348</th>\n      <td>SATTVA Classy.Elegant.Stylish Classic XXXL Bea...</td>\n      <td>stores</td>\n      <td>Amazon Fashion</td>\n      <td>https://m.media-amazon.com/images/I/71zaq78pO8...</td>\n      <td>https://www.amazon.in/Sattva-Classic-Filled-Be...</td>\n      <td>4.2</td>\n      <td>10</td>\n      <td>₹2,170</td>\n      <td>₹4,299</td>\n    </tr>\n    <tr>\n      <th>2349</th>\n      <td>Fastrack Glitch Analog Rose Gold Dial Women's ...</td>\n      <td>stores</td>\n      <td>Amazon Fashion</td>\n      <td>https://m.media-amazon.com/images/I/61B+fqeyzj...</td>\n      <td>https://www.amazon.in/Fastrack-Glitch-Analog-W...</td>\n      <td>3.5</td>\n      <td>16</td>\n      <td>NaN</td>\n      <td>₹5,595</td>\n    </tr>\n    <tr>\n      <th>2350</th>\n      <td>NAINVISH Women/Girl's Pure Cotton Paisley Prin...</td>\n      <td>stores</td>\n      <td>Amazon Fashion</td>\n      <td>https://m.media-amazon.com/images/I/81qATlsYzk...</td>\n      <td>https://www.amazon.in/NAINVISH-Cotton-Paisley-...</td>\n      <td>3.4</td>\n      <td>318</td>\n      <td>₹499</td>\n      <td>₹2,599</td>\n    </tr>\n    <tr>\n      <th>2351</th>\n      <td>Tupperware Aquasafe Xtreme Plastic Bottle, 1L,...</td>\n      <td>stores</td>\n      <td>Amazon Fashion</td>\n      <td>https://m.media-amazon.com/images/I/51z7AvKIml...</td>\n      <td>https://www.amazon.in/Tupperware-Aquasafe-Xtre...</td>\n      <td>4.6</td>\n      <td>2,741</td>\n      <td>₹666</td>\n      <td>₹695</td>\n    </tr>\n  </tbody>\n</table>\n<p>2352 rows × 9 columns</p>\n</div>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(csv_dfs[\"Amazon Fashion\"])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-26T07:25:53.771786Z",
     "start_time": "2024-06-26T07:25:53.742112800Z"
    }
   },
   "id": "c58a4a6de7b75851",
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "# Saving the images embeddeding"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e723bba182642670"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### embed the images"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3d6113965b408568"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# download image\n",
    "def download_image(url):\n",
    "    img_data = requests.get(url).content\n",
    "    return img_data"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-26T11:34:02.898682400Z",
     "start_time": "2024-06-26T11:34:02.889683Z"
    }
   },
   "id": "183e51e7928fafba",
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def embed_image_from_url(url):\n",
    "    img_data = download_image(url)\n",
    "    img = load_image_bytes(img_data)\n",
    "    embeddings = get_image_embeddings(img)\n",
    "    return embeddings"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-26T11:34:04.156795800Z",
     "start_time": "2024-06-26T11:34:04.108278900Z"
    }
   },
   "id": "bc0f90a5d9e74125",
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 424ms/step\n",
      "0 / 720\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 285ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 366ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 309ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 417ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 363ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 270ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 243ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 273ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 244ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 243ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 280ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 291ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 233ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 242ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 241ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 241ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 239ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 270ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 291ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 240ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 252ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 254ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 222ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 255ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 246ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 244ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 238ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 257ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 225ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 259ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 244ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 240ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 299ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 251ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 241ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 247ms/step\n",
      "36 / 720\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 247ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 234ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 236ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 248ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 267ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 249ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 247ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 263ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 230ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 264ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 251ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 234ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 266ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 231ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 255ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 262ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 230ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 227ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 243ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 236ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 236ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 253ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 255ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 295ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 240ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 340ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 429ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 330ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 371ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 283ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 322ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 293ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 244ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 370ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 456ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 326ms/step\n",
      "72 / 720\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 247ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 241ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 231ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 254ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 275ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 223ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 231ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 245ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 232ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 271ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 283ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 236ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 319ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 261ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 263ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 241ms/step\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[17], line 14\u001B[0m\n\u001B[0;32m     11\u001B[0m images_names \u001B[38;5;241m=\u001B[39m [\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mi\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.jpg\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;241m1\u001B[39m, \u001B[38;5;28mlen\u001B[39m(urls) \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m1\u001B[39m)]\n\u001B[0;32m     13\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i, url \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(urls):\n\u001B[1;32m---> 14\u001B[0m     embedding \u001B[38;5;241m=\u001B[39m \u001B[43membed_image_from_url\u001B[49m\u001B[43m(\u001B[49m\u001B[43murl\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     15\u001B[0m     np\u001B[38;5;241m.\u001B[39msave(f, embedding)\n\u001B[0;32m     16\u001B[0m     \u001B[38;5;66;03m# print progress every 5% of the way\u001B[39;00m\n",
      "Cell \u001B[1;32mIn[4], line 4\u001B[0m, in \u001B[0;36membed_image_from_url\u001B[1;34m(url)\u001B[0m\n\u001B[0;32m      2\u001B[0m img_data \u001B[38;5;241m=\u001B[39m download_image(url)\n\u001B[0;32m      3\u001B[0m img \u001B[38;5;241m=\u001B[39m load_image_bytes(img_data)\n\u001B[1;32m----> 4\u001B[0m embeddings \u001B[38;5;241m=\u001B[39m \u001B[43mget_image_embeddings\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimg\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      5\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m embeddings\n",
      "File \u001B[1;32mD:\\Documents\\GitHub\\Amazon-Image-matcher\\VGG.py:50\u001B[0m, in \u001B[0;36mget_image_embeddings\u001B[1;34m(object_image)\u001B[0m\n\u001B[0;32m     42\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m     43\u001B[0m \u001B[38;5;124;03m  -----------------------------------------------------\u001B[39;00m\n\u001B[0;32m     44\u001B[0m \u001B[38;5;124;03m  convert image into 3d array and add additional dimension for model input\u001B[39;00m\n\u001B[0;32m     45\u001B[0m \u001B[38;5;124;03m  -----------------------------------------------------\u001B[39;00m\n\u001B[0;32m     46\u001B[0m \u001B[38;5;124;03m  return embeddings of the given image\u001B[39;00m\n\u001B[0;32m     47\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m     49\u001B[0m image_array \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mexpand_dims(object_image, axis\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m)\n\u001B[1;32m---> 50\u001B[0m image_embedding \u001B[38;5;241m=\u001B[39m \u001B[43mvgg16\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpredict\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimage_array\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     51\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m image_embedding\n",
      "File \u001B[1;32mD:\\Documents\\GitHub\\Amazon-Image-matcher\\.venv\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:117\u001B[0m, in \u001B[0;36mfilter_traceback.<locals>.error_handler\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    115\u001B[0m filtered_tb \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m    116\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 117\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    118\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m    119\u001B[0m     filtered_tb \u001B[38;5;241m=\u001B[39m _process_traceback_frames(e\u001B[38;5;241m.\u001B[39m__traceback__)\n",
      "File \u001B[1;32mD:\\Documents\\GitHub\\Amazon-Image-matcher\\.venv\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:504\u001B[0m, in \u001B[0;36mTensorFlowTrainer.predict\u001B[1;34m(self, x, batch_size, verbose, steps, callbacks)\u001B[0m\n\u001B[0;32m    502\u001B[0m callbacks\u001B[38;5;241m.\u001B[39mon_predict_batch_begin(step)\n\u001B[0;32m    503\u001B[0m data \u001B[38;5;241m=\u001B[39m get_data(iterator)\n\u001B[1;32m--> 504\u001B[0m batch_outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpredict_function\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    505\u001B[0m outputs \u001B[38;5;241m=\u001B[39m append_to_outputs(batch_outputs, outputs)\n\u001B[0;32m    506\u001B[0m callbacks\u001B[38;5;241m.\u001B[39mon_predict_batch_end(step, {\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124moutputs\u001B[39m\u001B[38;5;124m\"\u001B[39m: batch_outputs})\n",
      "File \u001B[1;32mD:\\Documents\\GitHub\\Amazon-Image-matcher\\.venv\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001B[0m, in \u001B[0;36mfilter_traceback.<locals>.error_handler\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    148\u001B[0m filtered_tb \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m    149\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 150\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    151\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m    152\u001B[0m   filtered_tb \u001B[38;5;241m=\u001B[39m _process_traceback_frames(e\u001B[38;5;241m.\u001B[39m__traceback__)\n",
      "File \u001B[1;32mD:\\Documents\\GitHub\\Amazon-Image-matcher\\.venv\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:833\u001B[0m, in \u001B[0;36mFunction.__call__\u001B[1;34m(self, *args, **kwds)\u001B[0m\n\u001B[0;32m    830\u001B[0m compiler \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mxla\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jit_compile \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnonXla\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    832\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m OptionalXlaContext(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jit_compile):\n\u001B[1;32m--> 833\u001B[0m   result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwds\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    835\u001B[0m new_tracing_count \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mexperimental_get_tracing_count()\n\u001B[0;32m    836\u001B[0m without_tracing \u001B[38;5;241m=\u001B[39m (tracing_count \u001B[38;5;241m==\u001B[39m new_tracing_count)\n",
      "File \u001B[1;32mD:\\Documents\\GitHub\\Amazon-Image-matcher\\.venv\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:878\u001B[0m, in \u001B[0;36mFunction._call\u001B[1;34m(self, *args, **kwds)\u001B[0m\n\u001B[0;32m    875\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_lock\u001B[38;5;241m.\u001B[39mrelease()\n\u001B[0;32m    876\u001B[0m \u001B[38;5;66;03m# In this case we have not created variables on the first call. So we can\u001B[39;00m\n\u001B[0;32m    877\u001B[0m \u001B[38;5;66;03m# run the first trace but we should fail if variables are created.\u001B[39;00m\n\u001B[1;32m--> 878\u001B[0m results \u001B[38;5;241m=\u001B[39m \u001B[43mtracing_compilation\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcall_function\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    879\u001B[0m \u001B[43m    \u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkwds\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_variable_creation_config\u001B[49m\n\u001B[0;32m    880\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    881\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_created_variables:\n\u001B[0;32m    882\u001B[0m   \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCreating variables on a non-first call to a function\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    883\u001B[0m                    \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m decorated with tf.function.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[1;32mD:\\Documents\\GitHub\\Amazon-Image-matcher\\.venv\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compilation.py:139\u001B[0m, in \u001B[0;36mcall_function\u001B[1;34m(args, kwargs, tracing_options)\u001B[0m\n\u001B[0;32m    137\u001B[0m bound_args \u001B[38;5;241m=\u001B[39m function\u001B[38;5;241m.\u001B[39mfunction_type\u001B[38;5;241m.\u001B[39mbind(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m    138\u001B[0m flat_inputs \u001B[38;5;241m=\u001B[39m function\u001B[38;5;241m.\u001B[39mfunction_type\u001B[38;5;241m.\u001B[39munpack_inputs(bound_args)\n\u001B[1;32m--> 139\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunction\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_flat\u001B[49m\u001B[43m(\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# pylint: disable=protected-access\u001B[39;49;00m\n\u001B[0;32m    140\u001B[0m \u001B[43m    \u001B[49m\u001B[43mflat_inputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcaptured_inputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfunction\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcaptured_inputs\u001B[49m\n\u001B[0;32m    141\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mD:\\Documents\\GitHub\\Amazon-Image-matcher\\.venv\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\concrete_function.py:1322\u001B[0m, in \u001B[0;36mConcreteFunction._call_flat\u001B[1;34m(self, tensor_inputs, captured_inputs)\u001B[0m\n\u001B[0;32m   1318\u001B[0m possible_gradient_type \u001B[38;5;241m=\u001B[39m gradients_util\u001B[38;5;241m.\u001B[39mPossibleTapeGradientTypes(args)\n\u001B[0;32m   1319\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (possible_gradient_type \u001B[38;5;241m==\u001B[39m gradients_util\u001B[38;5;241m.\u001B[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001B[0;32m   1320\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m executing_eagerly):\n\u001B[0;32m   1321\u001B[0m   \u001B[38;5;66;03m# No tape is watching; skip to running the function.\u001B[39;00m\n\u001B[1;32m-> 1322\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_inference_function\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcall_preflattened\u001B[49m\u001B[43m(\u001B[49m\u001B[43margs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1323\u001B[0m forward_backward \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_select_forward_and_backward_functions(\n\u001B[0;32m   1324\u001B[0m     args,\n\u001B[0;32m   1325\u001B[0m     possible_gradient_type,\n\u001B[0;32m   1326\u001B[0m     executing_eagerly)\n\u001B[0;32m   1327\u001B[0m forward_function, args_with_tangents \u001B[38;5;241m=\u001B[39m forward_backward\u001B[38;5;241m.\u001B[39mforward()\n",
      "File \u001B[1;32mD:\\Documents\\GitHub\\Amazon-Image-matcher\\.venv\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:216\u001B[0m, in \u001B[0;36mAtomicFunction.call_preflattened\u001B[1;34m(self, args)\u001B[0m\n\u001B[0;32m    214\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcall_preflattened\u001B[39m(\u001B[38;5;28mself\u001B[39m, args: Sequence[core\u001B[38;5;241m.\u001B[39mTensor]) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Any:\n\u001B[0;32m    215\u001B[0m \u001B[38;5;250m  \u001B[39m\u001B[38;5;124;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001B[39;00m\n\u001B[1;32m--> 216\u001B[0m   flat_outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcall_flat\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    217\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfunction_type\u001B[38;5;241m.\u001B[39mpack_output(flat_outputs)\n",
      "File \u001B[1;32mD:\\Documents\\GitHub\\Amazon-Image-matcher\\.venv\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:251\u001B[0m, in \u001B[0;36mAtomicFunction.call_flat\u001B[1;34m(self, *args)\u001B[0m\n\u001B[0;32m    249\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m record\u001B[38;5;241m.\u001B[39mstop_recording():\n\u001B[0;32m    250\u001B[0m   \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_bound_context\u001B[38;5;241m.\u001B[39mexecuting_eagerly():\n\u001B[1;32m--> 251\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_bound_context\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcall_function\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    252\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    253\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mlist\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43margs\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    254\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mlen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfunction_type\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mflat_outputs\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    255\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    256\u001B[0m   \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    257\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m make_call_op_in_graph(\n\u001B[0;32m    258\u001B[0m         \u001B[38;5;28mself\u001B[39m,\n\u001B[0;32m    259\u001B[0m         \u001B[38;5;28mlist\u001B[39m(args),\n\u001B[0;32m    260\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_bound_context\u001B[38;5;241m.\u001B[39mfunction_call_options\u001B[38;5;241m.\u001B[39mas_attrs(),\n\u001B[0;32m    261\u001B[0m     )\n",
      "File \u001B[1;32mD:\\Documents\\GitHub\\Amazon-Image-matcher\\.venv\\Lib\\site-packages\\tensorflow\\python\\eager\\context.py:1500\u001B[0m, in \u001B[0;36mContext.call_function\u001B[1;34m(self, name, tensor_inputs, num_outputs)\u001B[0m\n\u001B[0;32m   1498\u001B[0m cancellation_context \u001B[38;5;241m=\u001B[39m cancellation\u001B[38;5;241m.\u001B[39mcontext()\n\u001B[0;32m   1499\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m cancellation_context \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m-> 1500\u001B[0m   outputs \u001B[38;5;241m=\u001B[39m \u001B[43mexecute\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mexecute\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1501\u001B[0m \u001B[43m      \u001B[49m\u001B[43mname\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdecode\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mutf-8\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1502\u001B[0m \u001B[43m      \u001B[49m\u001B[43mnum_outputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mnum_outputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1503\u001B[0m \u001B[43m      \u001B[49m\u001B[43minputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtensor_inputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1504\u001B[0m \u001B[43m      \u001B[49m\u001B[43mattrs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mattrs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1505\u001B[0m \u001B[43m      \u001B[49m\u001B[43mctx\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1506\u001B[0m \u001B[43m  \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1507\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m   1508\u001B[0m   outputs \u001B[38;5;241m=\u001B[39m execute\u001B[38;5;241m.\u001B[39mexecute_with_cancellation(\n\u001B[0;32m   1509\u001B[0m       name\u001B[38;5;241m.\u001B[39mdecode(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mutf-8\u001B[39m\u001B[38;5;124m\"\u001B[39m),\n\u001B[0;32m   1510\u001B[0m       num_outputs\u001B[38;5;241m=\u001B[39mnum_outputs,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1514\u001B[0m       cancellation_manager\u001B[38;5;241m=\u001B[39mcancellation_context,\n\u001B[0;32m   1515\u001B[0m   )\n",
      "File \u001B[1;32mD:\\Documents\\GitHub\\Amazon-Image-matcher\\.venv\\Lib\\site-packages\\tensorflow\\python\\eager\\execute.py:53\u001B[0m, in \u001B[0;36mquick_execute\u001B[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001B[0m\n\u001B[0;32m     51\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m     52\u001B[0m   ctx\u001B[38;5;241m.\u001B[39mensure_initialized()\n\u001B[1;32m---> 53\u001B[0m   tensors \u001B[38;5;241m=\u001B[39m \u001B[43mpywrap_tfe\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mTFE_Py_Execute\u001B[49m\u001B[43m(\u001B[49m\u001B[43mctx\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_handle\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevice_name\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mop_name\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     54\u001B[0m \u001B[43m                                      \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mattrs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnum_outputs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     55\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m core\u001B[38;5;241m.\u001B[39m_NotOkStatusException \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m     56\u001B[0m   \u001B[38;5;28;01mif\u001B[39;00m name \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "if not os.path.exists(\"images\"):\n",
    "    os.mkdir(\"images/\")\n",
    "    \n",
    "# .keep file to keep the directory in git\n",
    "with open(\"images/.keep\", 'w') as f:\n",
    "    pass\n",
    "\n",
    "with gzip.open('images/images.npy.gz', 'wb') as f:\n",
    "    file = \"Air Conditioners\"\n",
    "    urls = csv_dfs[file][\"image\"]\n",
    "    images_names = [f\"{i}.jpg\" for i in range(1, len(urls) + 1)]\n",
    "    \n",
    "    for i, url in enumerate(urls):\n",
    "        embedding = embed_image_from_url(url)\n",
    "        np.save(f, embedding)\n",
    "        # print progress every 5% of the way\n",
    "        if i % (len(urls) // 20) == 0:\n",
    "            print(f\"{i} / {len(urls)}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-26T11:45:27.526393Z",
     "start_time": "2024-06-26T11:44:54.606486Z"
    }
   },
   "id": "ef187fd3479b0620",
   "execution_count": 17
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Usage"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3191a1ad99e44f61"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 512)\n",
      "(1, 512)\n",
      "(1, 512)\n",
      "(1, 512)\n",
      "(1, 512)\n",
      "(1, 512)\n",
      "(1, 512)\n",
      "(1, 512)\n",
      "(1, 512)\n",
      "(1, 512)\n"
     ]
    }
   ],
   "source": [
    "# load the embeddings\n",
    "with gzip.open('images/images.npy.gz', 'rb') as f:\n",
    "    for _ in range(10):\n",
    "        arr = np.load(f)\n",
    "        print(arr.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-26T11:43:53.404947200Z",
     "start_time": "2024-06-26T11:43:53.387944300Z"
    }
   },
   "id": "4692a818acce4897",
   "execution_count": 16
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "# Preprocessing"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ee4cbeef3047cb3b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Amazon Fashion"
   ],
   "id": "7f7853ea851fd2a5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-26T07:26:20.563755100Z",
     "start_time": "2024-06-26T07:26:08.782767100Z"
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[8], line 3\u001B[0m\n\u001B[0;32m      1\u001B[0m amazon_fashion_df \u001B[38;5;241m=\u001B[39m csv_dfs[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAmazon Fashion\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n\u001B[0;32m      2\u001B[0m \u001B[38;5;66;03m# drop rows with missing values  # TODO: can discount be Nan?\u001B[39;00m\n\u001B[1;32m----> 3\u001B[0m amazon_fashion_df \u001B[38;5;241m=\u001B[39m \u001B[43mamazon_fashion_df\u001B[49m\u001B[38;5;241m.\u001B[39mdropna()\n\u001B[0;32m      4\u001B[0m \u001B[38;5;66;03m# drop duplicates\u001B[39;00m\n\u001B[0;32m      5\u001B[0m amazon_fashion_df \u001B[38;5;241m=\u001B[39m amazon_fashion_df\u001B[38;5;241m.\u001B[39mdrop_duplicates()\n",
      "File \u001B[1;32m_pydevd_bundle\\\\pydevd_cython_win32_312_64.pyx:1187\u001B[0m, in \u001B[0;36m_pydevd_bundle.pydevd_cython_win32_312_64.SafeCallWrapper.__call__\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32m_pydevd_bundle\\\\pydevd_cython_win32_312_64.pyx:627\u001B[0m, in \u001B[0;36m_pydevd_bundle.pydevd_cython_win32_312_64.PyDBFrame.trace_dispatch\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32m_pydevd_bundle\\\\pydevd_cython_win32_312_64.pyx:937\u001B[0m, in \u001B[0;36m_pydevd_bundle.pydevd_cython_win32_312_64.PyDBFrame.trace_dispatch\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32m_pydevd_bundle\\\\pydevd_cython_win32_312_64.pyx:928\u001B[0m, in \u001B[0;36m_pydevd_bundle.pydevd_cython_win32_312_64.PyDBFrame.trace_dispatch\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32m_pydevd_bundle\\\\pydevd_cython_win32_312_64.pyx:585\u001B[0m, in \u001B[0;36m_pydevd_bundle.pydevd_cython_win32_312_64.PyDBFrame.do_wait_suspend\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\PyCharm Professional\\plugins\\python\\helpers\\pydev\\pydevd.py:1184\u001B[0m, in \u001B[0;36mPyDB.do_wait_suspend\u001B[1;34m(self, thread, frame, event, arg, send_suspend_message, is_unhandled_exception)\u001B[0m\n\u001B[0;32m   1181\u001B[0m         from_this_thread\u001B[38;5;241m.\u001B[39mappend(frame_id)\n\u001B[0;32m   1183\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_threads_suspended_single_notification\u001B[38;5;241m.\u001B[39mnotify_thread_suspended(thread_id, stop_reason):\n\u001B[1;32m-> 1184\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_do_wait_suspend\u001B[49m\u001B[43m(\u001B[49m\u001B[43mthread\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mframe\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mevent\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43marg\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msuspend_type\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfrom_this_thread\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\PyCharm Professional\\plugins\\python\\helpers\\pydev\\pydevd.py:1199\u001B[0m, in \u001B[0;36mPyDB._do_wait_suspend\u001B[1;34m(self, thread, frame, event, arg, suspend_type, from_this_thread)\u001B[0m\n\u001B[0;32m   1196\u001B[0m             \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_mpl_hook()\n\u001B[0;32m   1198\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mprocess_internal_commands()\n\u001B[1;32m-> 1199\u001B[0m         \u001B[43mtime\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msleep\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m0.01\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1201\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcancel_async_evaluation(get_current_thread_id(thread), \u001B[38;5;28mstr\u001B[39m(\u001B[38;5;28mid\u001B[39m(frame)))\n\u001B[0;32m   1203\u001B[0m \u001B[38;5;66;03m# process any stepping instructions\u001B[39;00m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 8,
   "source": [
    "amazon_fashion_df = csv_dfs[\"Amazon Fashion\"]\n",
    "# drop rows with missing values  # TODO: can discount be Nan?\n",
    "amazon_fashion_df = amazon_fashion_df.dropna()\n",
    "# drop duplicates\n",
    "amazon_fashion_df = amazon_fashion_df.drop_duplicates()\n",
    "# drop rows with links that are not amazon links  # TODO: check if this is necessary\n",
    "amazon_fashion_df = amazon_fashion_df[amazon_fashion_df[\"link\"].str.contains(\"amazon.com\")]\n",
    "# drop rows with links that don't work  # FIXME: how?\n",
    "amazon_fashion_df = amazon_fashion_df[amazon_fashion_df[\"link\"].str.contains(\"404\")]\n",
    "# drop columns that are not needed\n",
    "amazon_fashion_df = amazon_fashion_df.drop(columns=[\"main_category\"])\n",
    "# normalize the ratings column\n",
    "amazon_fashion_df[\"rating\"] = amazon_fashion_df[\"rating\"] / 5.0"
   ],
   "id": "ad4ad41bde65dc11"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "---\n",
    "# Visualizing the data"
   ],
   "id": "6c3f16463776459e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Reality Check"
   ],
   "id": "4ac104ecdab78eac"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [],
   "id": "3125db6cd659f859"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Exploration"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1310d81c0ce2294e"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "c1b693c3b49111e3",
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
